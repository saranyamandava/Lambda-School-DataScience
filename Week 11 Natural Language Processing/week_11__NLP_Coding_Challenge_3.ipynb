{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week 11_ NLP Coding Challenge #3.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1chzJPrvPFOse888_DBrPA-hLQmWPx6ru",
          "timestamp": 1528902877357
        },
        {
          "file_id": "1NDYcKfAFVfU7JItJynnxb8lEAbVqpCl0",
          "timestamp": 1528401746750
        },
        {
          "file_id": "1xeSCCuNTKvZrNNmyRc1dBgiYsBJdrqOq",
          "timestamp": 1527930675172
        },
        {
          "file_id": "1sIBdz6SOq22toKGnIXG_pS7RGlXhVD9K",
          "timestamp": 1527911413882
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qd1ltfV8QShV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Coding Challenge #3: Natural Language Processing"
      ]
    },
    {
      "metadata": {
        "id": "iWWDecybQ1zz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this Coding Challenge, you will cover **Word2vec **which is a popular algorithm for building vector representations of words (i.e. word embeddings). The concept behind Word2Vec is quite straightforward - an assumption is made that the meaning of a word can be inferred by the *context it appears in* or *the company it keeps*. This is similar to stating: “tell me about your friends, and I will tell who you are”. \n",
        "\n",
        "If **2 **words  have very similar neighbors (meaning: the context in which it is used is similar), then the words are most likely quite similar.\n",
        "\n",
        "In this Coding Challenge, you will go through the process of training a Word2vec model with a sample set of documents and then examine certain attributes of the model. After that, you will train a Word2vec model with a large corpus of text and then ascertain the similarity among words in the corpus.\n"
      ]
    },
    {
      "metadata": {
        "id": "2U6ACjkamo-G",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "62a49c4d-3279-47f6-e225-b5517f043dac",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528913480327,
          "user_tz": 300,
          "elapsed": 2680,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://radimrehurek.com/gensim/install.html\n",
        "!pip install --upgrade gensim"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement not upgraded as not directly required: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.5.7)\n",
            "Requirement not upgraded as not directly required: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement not upgraded as not directly required: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.3)\n",
            "Requirement not upgraded as not directly required: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement not upgraded as not directly required: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
            "Requirement not upgraded as not directly required: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement not upgraded as not directly required: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.7.37)\n",
            "Requirement not upgraded as not directly required: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement not upgraded as not directly required: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement not upgraded as not directly required: botocore<1.11.0,>=1.10.37 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.10.37)\n",
            "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
            "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.37->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement not upgraded as not directly required: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.37->boto3->smart-open>=1.2.1->gensim) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6LHb47usm2_L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 4369
        },
        "outputId": "690f07fb-466b-4f95-c79c-629561425a03",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528913545594,
          "user_tz": 300,
          "elapsed": 59253,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /content/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    | Downloading package nps_chat to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package webtext to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "flAQZT_ZgEEk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #1: ** Tokenize the sample set of documents\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IoEKZI3SMJZM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "\n",
        "import gensim\n",
        "\n",
        "raw_content = ['The dog ran up the steps and entered the owner\\'s room to check if the owner was in the room.',\n",
        "             'My name is Thomson Comer, commander of the Machine Learning program at Lambda school.',\n",
        "             'I am creating the curriculum for the Machine Learning program and will be teaching the full-time Machine Learning program.',\n",
        "            'Machine Learning is one of my favorite subjects.',\n",
        "            'I am excited about taking the Machine Learning class at the Lambda school starting in April.',\n",
        "                'When does the Machine Learning program kick-off at Lambda school?',\n",
        "                'The batter hit the ball out off AT&T park into the pacific ocean.',\n",
        "                'The pitcher threw the ball into the dug-out.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aews-ibejOF7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #2: ** Train the Word2vec model with tokenized content; size of the word vectors is 5; the word should show-up at least once in the raw content"
      ]
    },
    {
      "metadata": {
        "id": "kvxuDVZay_yc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5ecdeec4-d62a-46a7-e1fb-e6b35a1c1440",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528913626054,
          "user_tz": 300,
          "elapsed": 572,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "657QHc_Rnw1J",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tokens = [nltk.word_tokenize(content) for content in raw_content]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5eksPEbIzfpy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(tokens, size=5, min_count=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wor0XODRoc0M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #3: **Output the number of words as well as the list of words in the model's vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "XbP2v2erouxO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9928fe8f-cfe9-4a30-9c43-aaac9c90fade",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528913943751,
          "user_tz": 300,
          "elapsed": 409,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (model)\n",
        "print (list(model.wv.vocab))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=69, size=5, alpha=0.025)\n",
            "['The', 'dog', 'ran', 'up', 'the', 'steps', 'and', 'entered', 'owner', \"'s\", 'room', 'to', 'check', 'if', 'was', 'in', '.', 'My', 'name', 'is', 'Thomson', 'Comer', ',', 'commander', 'of', 'Machine', 'Learning', 'program', 'at', 'Lambda', 'school', 'I', 'am', 'creating', 'curriculum', 'for', 'will', 'be', 'teaching', 'full-time', 'one', 'my', 'favorite', 'subjects', 'excited', 'about', 'taking', 'class', 'starting', 'April', 'When', 'does', 'kick-off', '?', 'batter', 'hit', 'ball', 'out', 'off', 'AT', '&', 'T', 'park', 'into', 'pacific', 'ocean', 'pitcher', 'threw', 'dug-out']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q0uQwqdLpcpk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #4: **Output the vector of words for the following tokens: **a)** curriculum, **b)** ocean, and **c) **pitcher"
      ]
    },
    {
      "metadata": {
        "id": "v86ElbNTp4TY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "17d5341b-a9b8-4f87-82b1-a80f7d082009",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528913878126,
          "user_tz": 300,
          "elapsed": 397,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv['ocean','curriculum','pitcher']  # numpy vector of a word"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.03142553, -0.05788845,  0.09867626, -0.05976471,  0.08484984],\n",
              "       [ 0.01484462, -0.05936503, -0.01785711,  0.0569173 ,  0.08448923],\n",
              "       [ 0.09187835, -0.06352206,  0.03383601,  0.09409906,  0.03991493]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "tzwJ_9R2q0qH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #5:** Now we are going to train the model with more data - larger corpus i.e. the 20 newsgroups text dataset. Fetch the data from the training subset\n",
        "\n",
        "*Reference*: http://scikit-learn.org/stable/datasets/index.html"
      ]
    },
    {
      "metadata": {
        "id": "2ZNmfKhit91S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c78b1bfe-2345-4bdf-f75c-853daf5c8f05",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528913815508,
          "user_tz": 300,
          "elapsed": 15555,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YmnUlfUMufpb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #6:** Output the metadata for the data that is fetched"
      ]
    },
    {
      "metadata": {
        "id": "wytJ9wL7u-Zu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a9cd544a-5bf3-4c96-9dda-ac7c2ab17152",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528914097325,
          "user_tz": 300,
          "elapsed": 375,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (dir(newsgroups_train))\n",
        "print (newsgroups_train.target_names)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yb54jbjPwIxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #7: ** Output the # of posts across the different categories"
      ]
    },
    {
      "metadata": {
        "id": "__SJ7enmwQtC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3b51f1be-8bed-458c-f3ca-e5967d0285a1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528914339867,
          "user_tz": 300,
          "elapsed": 640,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (newsgroups_train.description)\n",
        "print (newsgroups_train.target_names)\n",
        "print (len(newsgroups_train.data))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 20 newsgroups by date dataset\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "11314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ukQLf2PXwcwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #8**: Tokenize the body of text for each post"
      ]
    },
    {
      "metadata": {
        "id": "_rj_dddBw5_6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "token1 = [nltk.word_tokenize(post) for post in newsgroups_train.data] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AMRgRpvJxN0h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #9**: Train the Word2vec model - words should show up at least 3 times in the corpus of text\n",
        "and the size of each word vector is 200 (i.e. dimension = 200)\n",
        "\n",
        "Reference\" Scroll down to the section \"A closer look at the parameter settings\" to review the parameters that can be set"
      ]
    },
    {
      "metadata": {
        "id": "PInKDc0vxXr9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(token1, size=200, min_count=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cIg7JCdcxrCr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #10**:  List the number of words in the model's vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "iYEmDcwoxxCB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6c4d0186-8d93-4526-8da4-782213be58b5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528914924797,
          "user_tz": 300,
          "elapsed": 438,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (model)\n",
        "print (len(list(model.wv.vocab)))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=60064, size=200, alpha=0.025)\n",
            "60064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xF-RZy-1CRH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #11:** Examine word similarity to the word \"Christ\""
      ]
    },
    {
      "metadata": {
        "id": "hoYABV8E5FgI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2419c87c-00d1-428b-cb68-266c5a883aa5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528914989929,
          "user_tz": 300,
          "elapsed": 365,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('christ')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Christ', 0.6241304874420166),\n",
              " ('God', 0.6131706237792969),\n",
              " ('damnation', 0.5896145105361938),\n",
              " ('Yeah', 0.575957715511322),\n",
              " ('Father', 0.5713233351707458),\n",
              " ('Lord', 0.5701766014099121),\n",
              " ('Jesus', 0.5599207878112793),\n",
              " ('Matthew', 0.5572882890701294),\n",
              " ('Ah', 0.5506866574287415),\n",
              " ('Heaven', 0.5497388243675232)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "0IaDtaKEb2yI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "bd6df359-6bb4-4088-fb54-b44337a30252",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528915007920,
          "user_tz": 300,
          "elapsed": 367,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('Christ')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Jesus', 0.8998662233352661),\n",
              " ('God', 0.8627258539199829),\n",
              " ('Lord', 0.8626646995544434),\n",
              " ('Father', 0.8608936071395874),\n",
              " ('sin', 0.8307104706764221),\n",
              " ('Spirit', 0.8138176202774048),\n",
              " ('Son', 0.8119875192642212),\n",
              " ('Satan', 0.8077149391174316),\n",
              " ('Him', 0.7846865653991699),\n",
              " ('heaven', 0.7837395071983337)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "IPOj0D8PDCBh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #12**: Examine document similarity with Doc2vec to any body of text of your choice\n",
        "\n",
        "*Reference*: https://radimrehurek.com/gensim/models/doc2vec.html"
      ]
    },
    {
      "metadata": {
        "id": "7ZiQtRgCtFmL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    sentence = []\n",
        "    for w in words:\n",
        "      if (w.isalpha()):\n",
        "        sentence.append(w)\n",
        "    return sentence    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D1QS5OoVEKjD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "68336551-6a7f-4019-a5d0-be8f89a70765",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528919715135,
          "user_tz": 300,
          "elapsed": 74310,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# We need to train a doc2vec model with the 20 Newsgroup dataset\n",
        "# One of arguments to the model is a \"TaggedDocument\" so we will first go ahead and create a Tagged document\n",
        "\n",
        "# Import TaggedDocument\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "# Tokenize each of the posts within the newsgroups\n",
        "sentences = [preprocess_text(doc) for doc in newsgroups_train.data] \n",
        "\n",
        "# Create a list of tagged_documents\n",
        "# Every item within the tagged_documents list is a tokenized version of the posts \n",
        "tagged_documents_list = []\n",
        "for i, sent in enumerate(sentences):\n",
        "    tagged_documents_list.append(TaggedDocument(sent, [\"sent_{}\".format(i)]))\n",
        "\n",
        "# Examine the first item within tagged_document_lists\n",
        "# print(tagged_documents_list[0])\n",
        "\n",
        "# Train the model with the list of Tagged Documents\n",
        "# size of the vector is 300\n",
        "doc2vec_model = gensim.models.doc2vec.Doc2Vec(tagged_documents_list,\n",
        "                                              vector_size=300)\n",
        "\n",
        "# Get the vector representation for a new document\n",
        "vec_representation = doc2vec_model.infer_vector('I love test driving luxury cars.'.split())\n",
        "#print(vec_representation)\n",
        "\n",
        "#Determine the documents (posts) similar to the new document/post\n",
        "similar = doc2vec_model.docvecs.most_similar([vec_representation])\n",
        "print (similar)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('sent_2951', 0.6760349273681641), ('sent_8485', 0.6438456177711487), ('sent_573', 0.6398172974586487), ('sent_10489', 0.6350080966949463), ('sent_10434', 0.6185758709907532), ('sent_3759', 0.6132775545120239), ('sent_1871', 0.6117489337921143), ('sent_10479', 0.6094856858253479), ('sent_6852', 0.6090832352638245), ('sent_9456', 0.6080637574195862)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hrJvs1E0Gg8Y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "20bfd0c8-e1e5-46c2-90ef-fc7767b3073c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528919737916,
          "user_tz": 300,
          "elapsed": 551,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Examine the first document in the list above to gauge the similarity\n",
        "print (tagged_documents_list[2951])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TaggedDocument(['From', 'Brent', 'Stone', 'Subject', 'Wanted', 'Advice', 'for', 'New', 'Cylist', 'Ditto', 'Organization', 'University', 'of', 'Virginia', 'Lines', 'In', 'article', 'blaisec', 'Blaise', 'Cirelli', 'writes', 'I', 'thinking', 'about', 'becoming', 'a', 'bike', 'owner', 'this', 'year', 'any', 'bike', 'experience', 'thus', 'far', 'I', 'figure', 'that', 'getting', 'a', 'decent', 'used', 'bike', 'for', 'under', 'the', 'thing', 'would', 'pay', 'for', 'itself', 'while', 'I', 'at', 'grad', 'school', 'car', 'permits', 'are', 'where', 'I', 'going', 'and', 'who', 'want', 'to', 'ride', 'a', 'bus', 'I', 'looking', 'for', 'advice', 'on', 'a', 'first', 'bike', 'best', 'I', 'NOT', 'looking', 'for', 'an', 'old', 'loud', 'roaring', 'thing', 'that', 'sounds', 'like', 'a', 'monster', 'The', 'quit', 'whirring', 'of', 'newer', 'engines', 'is', 'more', 'to', 'my', 'liking', 'Apprec', 'any', 'advice', 'Thanks', 'BS'], ['sent_2951'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RtiIzORkpTes",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Stretch Goal: **\n",
        "\n",
        "Download the pre-trained word vectors from Google. Access the pre-trained vectors via the following link: https://code.google.com/archive/p/word2vec\n",
        "\n",
        "Load the pre-trained word vectors and train the **Word2vec** model\n",
        "\n",
        "Examine the first 100 keys or words of the vocabulary\n",
        "\n",
        "Outputs the vector representation for a select set of words - the words can be of your choice\n",
        "\n",
        "Examine the similarity between words - the words can be of your choice\n",
        "\n",
        "For example: \n",
        "\n",
        "model.similarity('house', 'bungalow')\n",
        "\n",
        "model.similarity('house', 'umbrella')\n"
      ]
    }
  ]
}