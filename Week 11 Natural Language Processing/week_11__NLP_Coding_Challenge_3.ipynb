{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week 11_ NLP Coding Challenge #3.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1chzJPrvPFOse888_DBrPA-hLQmWPx6ru",
          "timestamp": 1528902877357
        },
        {
          "file_id": "1NDYcKfAFVfU7JItJynnxb8lEAbVqpCl0",
          "timestamp": 1528401746750
        },
        {
          "file_id": "1xeSCCuNTKvZrNNmyRc1dBgiYsBJdrqOq",
          "timestamp": 1527930675172
        },
        {
          "file_id": "1sIBdz6SOq22toKGnIXG_pS7RGlXhVD9K",
          "timestamp": 1527911413882
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qd1ltfV8QShV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Coding Challenge #3: Natural Language Processing"
      ]
    },
    {
      "metadata": {
        "id": "iWWDecybQ1zz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this Coding Challenge, you will cover **Word2vec **which is a popular algorithm for building vector representations of words (i.e. word embeddings). The concept behind Word2Vec is quite straightforward - an assumption is made that the meaning of a word can be inferred by the *context it appears in* or *the company it keeps*. This is similar to stating: “tell me about your friends, and I will tell who you are”. \n",
        "\n",
        "If **2 **words  have very similar neighbors (meaning: the context in which it is used is similar), then the words are most likely quite similar.\n",
        "\n",
        "In this Coding Challenge, you will go through the process of training a Word2vec model with a sample set of documents and then examine certain attributes of the model. After that, you will train a Word2vec model with a large corpus of text and then ascertain the similarity among words in the corpus.\n"
      ]
    },
    {
      "metadata": {
        "id": "2U6ACjkamo-G",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "d91376ca-f13f-4f25-ec90-859ac61efefc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988352822,
          "user_tz": 300,
          "elapsed": 10210,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# https://radimrehurek.com/gensim/install.html\n",
        "!pip install --upgrade gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/33/df6cb7acdcec5677ed130f4800f67509d24dbec74a03c329fcbf6b0864f0/gensim-3.4.0-cp36-cp36m-manylinux1_x86_64.whl (22.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 22.6MB 2.0MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement not upgraded as not directly required: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement not upgraded as not directly required: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.3)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/69/c92661a333f733510628f28b8282698b62cdead37291c8491f3271677c02/smart_open-1.5.7.tar.gz\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/b7/a88a67002b1185ed9a8e8a6ef15266728c2361fcb4f1d02ea331e4c7741d/boto-2.48.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 15.7MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement not upgraded as not directly required: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/60/03977a02a8a2c0a41f1bba7178bfb334c6784d45dd849019c1c9fa4c6f92/boto3-1.7.38-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 27.3MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
            "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Collecting botocore<1.11.0,>=1.10.38 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/32/3656404fd2b5714192c41aa8bc35c78d0efaf3732480a40f7c56a8af8db1/botocore-1.10.38-py2.py3-none-any.whl (4.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.3MB 10.4MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.6MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting docutils>=0.10 (from botocore<1.11.0,>=1.10.38->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 26.9MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.38->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Building wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/b1/9e/7d/bb3d3b55c597e72617140a0638c06382a5f17283881eae163e\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.48.0 boto3-1.7.38 botocore-1.10.38 bz2file-0.98 docutils-0.14 gensim-3.4.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.5.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6LHb47usm2_L",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 4369
        },
        "outputId": "593b0485-fd21-468b-cbb0-51bde56dfde4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988543019,
          "user_tz": 300,
          "elapsed": 95415,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /content/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /content/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package webtext to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /content/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "flAQZT_ZgEEk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #1: ** Tokenize the sample set of documents\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IoEKZI3SMJZM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "\n",
        "import gensim\n",
        "\n",
        "raw_content = ['The dog ran up the steps and entered the owner\\'s room to check if the owner was in the room.',\n",
        "             'My name is Thomson Comer, commander of the Machine Learning program at Lambda school.',\n",
        "             'I am creating the curriculum for the Machine Learning program and will be teaching the full-time Machine Learning program.',\n",
        "            'Machine Learning is one of my favorite subjects.',\n",
        "            'I am excited about taking the Machine Learning class at the Lambda school starting in April.',\n",
        "                'When does the Machine Learning program kick-off at Lambda school?',\n",
        "                'The batter hit the ball out off AT&T park into the pacific ocean.',\n",
        "                'The pitcher threw the ball into the dug-out.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aews-ibejOF7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #2: ** Train the Word2vec model with tokenized content; size of the word vectors is 5; the word should show-up at least once in the raw content"
      ]
    },
    {
      "metadata": {
        "id": "kvxuDVZay_yc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f5d87816-98ae-4c2a-ba71-ab48586f97e9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988725857,
          "user_tz": 300,
          "elapsed": 343,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "657QHc_Rnw1J",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tokens = [nltk.word_tokenize(content) for content in raw_content]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5eksPEbIzfpy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(tokens, size=5, min_count=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wor0XODRoc0M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #3: **Output the number of words as well as the list of words in the model's vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "XbP2v2erouxO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "28f108b6-d15f-4d1a-cdf1-e57062e44a72",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988732725,
          "user_tz": 300,
          "elapsed": 387,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (model)\n",
        "print (list(model.wv.vocab))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=69, size=5, alpha=0.025)\n",
            "['The', 'dog', 'ran', 'up', 'the', 'steps', 'and', 'entered', 'owner', \"'s\", 'room', 'to', 'check', 'if', 'was', 'in', '.', 'My', 'name', 'is', 'Thomson', 'Comer', ',', 'commander', 'of', 'Machine', 'Learning', 'program', 'at', 'Lambda', 'school', 'I', 'am', 'creating', 'curriculum', 'for', 'will', 'be', 'teaching', 'full-time', 'one', 'my', 'favorite', 'subjects', 'excited', 'about', 'taking', 'class', 'starting', 'April', 'When', 'does', 'kick-off', '?', 'batter', 'hit', 'ball', 'out', 'off', 'AT', '&', 'T', 'park', 'into', 'pacific', 'ocean', 'pitcher', 'threw', 'dug-out']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q0uQwqdLpcpk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #4: **Output the vector of words for the following tokens: **a)** curriculum, **b)** ocean, and **c) **pitcher"
      ]
    },
    {
      "metadata": {
        "id": "v86ElbNTp4TY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e3b7adbb-59ae-4123-bae3-464eb1f7d67c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988736463,
          "user_tz": 300,
          "elapsed": 1067,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv['ocean','curriculum','pitcher']  # numpy vector of a word"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01658506, -0.05433043, -0.05993745, -0.06291358,  0.09465564],\n",
              "       [ 0.0774902 , -0.00701892, -0.04136961, -0.05101689,  0.04262919],\n",
              "       [ 0.0024625 ,  0.08436355,  0.08406299, -0.04252343,  0.0677102 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "tzwJ_9R2q0qH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #5:** Now we are going to train the model with more data - larger corpus i.e. the 20 newsgroups text dataset. Fetch the data from the training subset\n",
        "\n",
        "*Reference*: http://scikit-learn.org/stable/datasets/index.html"
      ]
    },
    {
      "metadata": {
        "id": "2ZNmfKhit91S",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5007c565-77eb-4951-e9b0-da373cb7d78a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988757238,
          "user_tz": 300,
          "elapsed": 17345,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YmnUlfUMufpb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #6:** Output the metadata for the data that is fetched"
      ]
    },
    {
      "metadata": {
        "id": "wytJ9wL7u-Zu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d328473e-f64c-4905-96ce-0477618dcb17",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988764831,
          "user_tz": 300,
          "elapsed": 348,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (dir(newsgroups_train))\n",
        "print (newsgroups_train.target_names)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yb54jbjPwIxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #7: ** Output the # of posts across the different categories"
      ]
    },
    {
      "metadata": {
        "id": "__SJ7enmwQtC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "78fa8812-a462-4b1d-cd13-1ca6309735c6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988767856,
          "user_tz": 300,
          "elapsed": 339,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (newsgroups_train.description)\n",
        "print (newsgroups_train.target_names)\n",
        "print (len(newsgroups_train.data))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 20 newsgroups by date dataset\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "11314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ukQLf2PXwcwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #8**: Tokenize the body of text for each post"
      ]
    },
    {
      "metadata": {
        "id": "_rj_dddBw5_6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "token1 = [nltk.word_tokenize(post) for post in newsgroups_train.data] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AMRgRpvJxN0h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #9**: Train the Word2vec model - words should show up at least 3 times in the corpus of text\n",
        "and the size of each word vector is 200 (i.e. dimension = 200)\n",
        "\n",
        "Reference\" Scroll down to the section \"A closer look at the parameter settings\" to review the parameters that can be set"
      ]
    },
    {
      "metadata": {
        "id": "PInKDc0vxXr9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(token1, size=200, min_count=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cIg7JCdcxrCr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #10**:  List the number of words in the model's vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "iYEmDcwoxxCB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d08d347d-bcb7-4f32-de2d-8551763fd673",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988862374,
          "user_tz": 300,
          "elapsed": 379,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print (model)\n",
        "print (len(list(model.wv.vocab)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=60064, size=200, alpha=0.025)\n",
            "60064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xF-RZy-1CRH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #11:** Examine word similarity to the word \"Christ\""
      ]
    },
    {
      "metadata": {
        "id": "hoYABV8E5FgI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "610d40e8-ebf9-473b-8a87-097879488923",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988866207,
          "user_tz": 300,
          "elapsed": 340,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('christ')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('yeah', 0.6495965719223022),\n",
              " ('Ah', 0.598802924156189),\n",
              " ('Uh', 0.5683927536010742),\n",
              " ('Oops', 0.5622634291648865),\n",
              " ('Yeah', 0.5545791983604431),\n",
              " ('chuckling', 0.5344535708427429),\n",
              " ('Oh', 0.5340321063995361),\n",
              " ('Sister', 0.5179696083068848),\n",
              " ('Mr.Davidian', 0.5105739235877991),\n",
              " ('sh*t.', 0.5100235939025879)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "0IaDtaKEb2yI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "98bd6366-a6ac-4b3a-8f6a-b882a87a8e96",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988869482,
          "user_tz": 300,
          "elapsed": 365,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('Christ')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Jesus', 0.895038366317749),\n",
              " ('Lord', 0.8624118566513062),\n",
              " ('Father', 0.862071692943573),\n",
              " ('God', 0.8532782196998596),\n",
              " ('Spirit', 0.8234806060791016),\n",
              " ('sin', 0.8162057995796204),\n",
              " ('Son', 0.8147610425949097),\n",
              " ('heaven', 0.8008133172988892),\n",
              " ('His', 0.7844675779342651),\n",
              " ('faith', 0.7742908596992493)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "IPOj0D8PDCBh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step #12**: Examine document similarity with Doc2vec to any body of text of your choice\n",
        "\n",
        "*Reference*: https://radimrehurek.com/gensim/models/doc2vec.html"
      ]
    },
    {
      "metadata": {
        "id": "7ZiQtRgCtFmL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    sentence = []\n",
        "    for w in words:\n",
        "      if (w.isalpha()):\n",
        "        sentence.append(w)\n",
        "    return sentence    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D1QS5OoVEKjD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3bed4d7c-5783-4e34-faa9-5e2a71bb673e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528988979741,
          "user_tz": 300,
          "elapsed": 71792,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# We need to train a doc2vec model with the 20 Newsgroup dataset\n",
        "# One of arguments to the model is a \"TaggedDocument\" so we will first go ahead and create a Tagged document\n",
        "\n",
        "# Import TaggedDocument\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "# Tokenize each of the posts within the newsgroups\n",
        "sentences = [preprocess_text(doc) for doc in newsgroups_train.data] \n",
        "\n",
        "# Create a list of tagged_documents\n",
        "# Every item within the tagged_documents list is a tokenized version of the posts \n",
        "tagged_documents_list = []\n",
        "for i, sent in enumerate(sentences):\n",
        "    tagged_documents_list.append(TaggedDocument(sent, [\"sent_{}\".format(i)]))\n",
        "\n",
        "# Examine the first item within tagged_document_lists\n",
        "# print(tagged_documents_list[0])\n",
        "\n",
        "# Train the model with the list of Tagged Documents\n",
        "# size of the vector is 300\n",
        "doc2vec_model = gensim.models.doc2vec.Doc2Vec(tagged_documents_list,\n",
        "                                              vector_size=300)\n",
        "\n",
        "# Get the vector representation for a new document\n",
        "vec_representation = doc2vec_model.infer_vector('I love test driving luxury cars.'.split())\n",
        "#print(vec_representation)\n",
        "\n",
        "#Determine the documents (posts) similar to the new document/post\n",
        "similar = doc2vec_model.docvecs.most_similar([vec_representation])\n",
        "print (similar)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('sent_1131', 0.7032564878463745), ('sent_6974', 0.6784844994544983), ('sent_7621', 0.6773320436477661), ('sent_5863', 0.6692783236503601), ('sent_10391', 0.6660875082015991), ('sent_7767', 0.656580924987793), ('sent_2281', 0.6556556224822998), ('sent_1059', 0.6546320915222168), ('sent_5024', 0.6490312218666077), ('sent_7578', 0.6486107110977173)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hrJvs1E0Gg8Y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3c0dfd37-f83a-45e6-bc23-c908aa637272",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528989006139,
          "user_tz": 300,
          "elapsed": 223,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Examine the first document in the list above to gauge the similarity\n",
        "print (tagged_documents_list[1131])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TaggedDocument(['From', 'dowdy', 'Dowdy', 'Jackson', 'Subject', 'Re', 'Swimming', 'pool', 'defense', 'Organization', 'Northwestern', 'University', 'Evanston', 'Illinois', 'Lines', 'In', 'article', 'kbanaian', 'King', 'Banaian', 'writes', 'In', 'article', 'Ted', 'Frank', 'writes', 'In', 'article', 'dasmith', 'David', 'Smith', 'writes', 'Granted', 'the', 'simple', 'fact', 'of', 'holding', 'down', 'a', 'job', 'will', 'improve', 'these', 'kids', 'chances', 'of', 'getting', 'another', 'job', 'in', 'the', 'future', 'but', 'what', 'inner', 'city', 'kid', 'would', 'want', 'to', 'hold', 'down', 'just', 'one', 'more', 'minimum', 'wage', 'job', 'when', 'there', 'is', 'so', 'much', 'more', 'money', 'to', 'be', 'made', 'dealing', 'drugs', 'What', 'suburban', 'kid', 'would', 'want', 'to', 'hold', 'down', 'a', 'minimum', 'wage', 'job', 'when', 'there', 'is', 'so', 'much', 'more', 'money', 'to', 'be', 'made', 'dealing', 'drugs', 'Yet', 'somehow', 'surburban', 'kids', 'do', 'hold', 'down', 'minimum', 'wage', 'jobs', 'So', 'do', 'inner', 'city', 'kids', 'when', 'give', 'the', 'chance', 'Any', 'reason', 'you', 'think', 'that', 'inner', 'city', 'kids', 'are', 'incapable', 'of', 'doing', 'legitimate', 'work', 'I', 'suppose', 'the', 'correct', 'answer', 'is', 'not', 'family', 'values', 'not', 'Never', 'mind', 'Sorry', 'Are', 'you', 'assuming', 'that', 'families', 'in', 'the', 'inner', 'city', 'do', 'have', 'family', 'values', 'I', 'sure', 'hope', 'not'], ['sent_1131'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RtiIzORkpTes",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Stretch Goal: **\n",
        "\n",
        "Download the pre-trained word vectors from Google. Access the pre-trained vectors via the following link: https://code.google.com/archive/p/word2vec\n",
        "\n",
        "Load the pre-trained word vectors and train the **Word2vec** model\n",
        "\n",
        "Examine the first 100 keys or words of the vocabulary\n",
        "\n",
        "Outputs the vector representation for a select set of words - the words can be of your choice\n",
        "\n",
        "Examine the similarity between words - the words can be of your choice\n",
        "\n",
        "For example: \n",
        "\n",
        "model.similarity('house', 'bungalow')\n",
        "\n",
        "model.similarity('house', 'umbrella')\n"
      ]
    },
    {
      "metadata": {
        "id": "ow_kWOxL6yhs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "247579bb-e82b-4419-d535-92988c453e28",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528990219938,
          "user_tz": 300,
          "elapsed": 2816,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#https://github.com/ndrplz/google-drive-downloader\n",
        "!pip install googledrivedownloader"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googledrivedownloader\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/41/d59b2a5fcc7afeb40f23091694bd6e6a63ad118c93f834353ee5100285d5/googledrivedownloader-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: googledrivedownloader\n",
            "Successfully installed googledrivedownloader-0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n14qp5sZ6ztH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ceb0ac92-a26f-42fb-d6ff-d5ebca1a95c3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528990402449,
          "user_tz": 300,
          "elapsed": 17785,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='0B7XkCwpI5KDYNlNUTTlSS21pQmM',\n",
        "                                    dest_path='./data/GoogleNews-vectors-negative300.bin.gz')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 0B7XkCwpI5KDYNlNUTTlSS21pQmM into ./data/GoogleNews-vectors-negative300.bin.gz... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FoL4yXna7c_B",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HfiW9Pz97r0I",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9a873d77-09e9-429d-cc2d-a3a410b46392",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528991652482,
          "user_tz": 300,
          "elapsed": 622,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(list(model.vocab)[:100])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said', 'was', 'the', 'at', 'not', 'as', 'it', 'be', 'from', 'by', 'are', 'I', 'have', 'he', 'will', 'has', '####', 'his', 'an', 'this', 'or', 'their', 'who', 'they', 'but', '$', 'had', 'year', 'were', 'we', 'more', '###', 'up', 'been', 'you', 'its', 'one', 'about', 'would', 'which', 'out', 'can', 'It', 'all', 'also', 'two', 'after', 'first', 'He', 'do', 'time', 'than', 'when', 'We', 'over', 'last', 'new', 'other', 'her', 'people', 'into', 'In', 'our', 'there', 'A', 'she', 'could', 'just', 'years', 'some', 'U.S.', 'three', 'million', 'them', 'what', 'But', 'so', 'no', 'like', 'if', 'only', 'percent', 'get', 'did', 'him', 'game', 'back', 'because', 'now', '#.#', 'before']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ktZhITW1AXE7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "484d2d91-e49b-4439-d965-b350ecbad711",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528991688345,
          "user_tz": 300,
          "elapsed": 358,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print('house-bungalow similarity:', model.similarity('house', 'bungalow'))\n",
        "print('house-umbrella similarity:', model.similarity('house', 'umbrella'))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "house-bungalow similarity: 0.6878559817059837\n",
            "house-umbrella similarity: 0.1358489851424372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LfeOBtHvAdE-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1054
        },
        "outputId": "c71d0c51-9622-40ce-c13a-0845560d8ea7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528991731410,
          "user_tz": 300,
          "elapsed": 550,
          "user": {
            "displayName": "saranya Mandava",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "111547944025536722337"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model['The']"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.17285156,  0.27929688,  0.10693359, -0.15820312, -0.08447266,\n",
              "        0.05908203,  0.04077148,  0.00254822,  0.25976562,  0.18066406,\n",
              "        0.09765625, -0.08105469, -0.01049805,  0.09814453,  0.00060272,\n",
              "        0.07080078, -0.015625  , -0.09521484, -0.08105469, -0.02868652,\n",
              "       -0.03320312,  0.16503906,  0.03979492, -0.03710938,  0.04101562,\n",
              "       -0.12695312, -0.12890625,  0.12353516,  0.04980469,  0.01257324,\n",
              "        0.05786133, -0.00830078, -0.02832031, -0.03320312,  0.16113281,\n",
              "        0.07519531, -0.25976562,  0.08935547,  0.13574219,  0.00460815,\n",
              "       -0.04418945,  0.02319336, -0.10449219, -0.05151367,  0.08349609,\n",
              "       -0.02050781, -0.02172852, -0.02734375,  0.16015625,  0.19042969,\n",
              "       -0.0324707 ,  0.06787109,  0.10302734, -0.25390625,  0.00634766,\n",
              "        0.20507812,  0.02111816, -0.21679688, -0.02441406,  0.17089844,\n",
              "       -0.21875   ,  0.10009766, -0.15527344, -0.12597656, -0.03833008,\n",
              "       -0.05419922,  0.19238281,  0.21777344,  0.12109375, -0.02648926,\n",
              "        0.05297852, -0.0201416 ,  0.0534668 ,  0.07666016,  0.0456543 ,\n",
              "        0.01977539,  0.12451172,  0.10205078,  0.15234375,  0.25195312,\n",
              "        0.04296875, -0.18554688, -0.07519531,  0.22753906, -0.13183594,\n",
              "       -0.27539062, -0.20117188,  0.12597656, -0.18652344, -0.13964844,\n",
              "       -0.12597656,  0.08007812, -0.03393555, -0.23730469, -0.00765991,\n",
              "        0.28320312, -0.01721191,  0.02722168, -0.234375  ,  0.12890625,\n",
              "        0.16796875, -0.01019287,  0.06542969, -0.1328125 ,  0.07763672,\n",
              "       -0.11669922,  0.01464844, -0.05297852, -0.12109375, -0.09912109,\n",
              "        0.0559082 , -0.04199219,  0.10742188,  0.09130859,  0.14746094,\n",
              "        0.10205078, -0.00139618,  0.20117188,  0.08447266,  0.07568359,\n",
              "       -0.24316406, -0.18945312, -0.07910156, -0.06640625,  0.12792969,\n",
              "        0.15039062, -0.16601562, -0.09521484,  0.09619141, -0.05029297,\n",
              "       -0.02502441, -0.04931641, -0.04370117, -0.0234375 ,  0.06640625,\n",
              "       -0.13378906, -0.25      , -0.2421875 , -0.11328125,  0.02636719,\n",
              "        0.13867188, -0.07080078, -0.22265625, -0.11279297, -0.01806641,\n",
              "       -0.19042969,  0.07421875, -0.16601562,  0.00628662,  0.1953125 ,\n",
              "       -0.00436401, -0.16699219,  0.14746094, -0.04882812,  0.1875    ,\n",
              "       -0.234375  ,  0.14453125,  0.00769043,  0.04101562, -0.06738281,\n",
              "       -0.01672363, -0.05029297,  0.08496094,  0.03662109,  0.13476562,\n",
              "        0.08056641, -0.04467773,  0.16601562, -0.22265625,  0.00933838,\n",
              "       -0.11962891, -0.06494141,  0.03344727, -0.09765625,  0.01464844,\n",
              "        0.12158203,  0.12353516, -0.13476562, -0.05297852, -0.27734375,\n",
              "       -0.0378418 , -0.07910156, -0.06884766,  0.05712891,  0.03112793,\n",
              "        0.07080078,  0.07958984,  0.00656128,  0.14257812, -0.08447266,\n",
              "        0.03125   , -0.03564453,  0.02307129, -0.01660156, -0.17285156,\n",
              "       -0.16113281, -0.10742188,  0.05566406, -0.09228516, -0.25585938,\n",
              "        0.03930664, -0.20410156,  0.08496094,  0.04614258,  0.01208496,\n",
              "       -0.01257324,  0.06396484,  0.2578125 ,  0.18457031, -0.0559082 ,\n",
              "        0.0234375 ,  0.10498047,  0.12011719,  0.00909424, -0.00567627,\n",
              "        0.11132812,  0.15136719,  0.0246582 , -0.03833008,  0.11572266,\n",
              "        0.171875  , -0.04125977,  0.10302734,  0.04467773, -0.01269531,\n",
              "       -0.08203125, -0.03491211,  0.00787354,  0.07910156, -0.00050354,\n",
              "        0.02246094, -0.16113281,  0.11621094,  0.25195312, -0.04663086,\n",
              "       -0.08007812, -0.09863281,  0.04760742, -0.06054688,  0.11865234,\n",
              "       -0.03393555, -0.06689453, -0.0007782 , -0.04882812, -0.01190186,\n",
              "       -0.08251953,  0.21777344, -0.00180817,  0.1484375 , -0.02050781,\n",
              "        0.26367188,  0.17089844, -0.04638672,  0.12792969,  0.0480957 ,\n",
              "       -0.03759766,  0.16699219, -0.19140625,  0.0390625 ,  0.07714844,\n",
              "       -0.01586914,  0.09912109, -0.15820312, -0.01379395,  0.01855469,\n",
              "        0.15527344, -0.18164062,  0.03369141, -0.0008049 ,  0.06689453,\n",
              "       -0.01843262,  0.14160156,  0.06445312, -0.00732422,  0.07958984,\n",
              "       -0.0279541 ,  0.125     , -0.23730469,  0.03637695, -0.046875  ,\n",
              "       -0.046875  ,  0.05493164,  0.07958984,  0.27148438,  0.22558594,\n",
              "       -0.20507812, -0.20507812, -0.06445312, -0.07763672, -0.06982422,\n",
              "       -0.0177002 , -0.12890625,  0.02197266,  0.01477051, -0.05297852,\n",
              "       -0.203125  ,  0.06176758,  0.12304688,  0.12988281, -0.18261719],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}